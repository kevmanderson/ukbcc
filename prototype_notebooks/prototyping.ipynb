{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import process, fuzz\n",
    "\n",
    "def queryHospitalData(pathToMainDataset: str, searchCodeDict: dict) -> list:\n",
    "    \"\"\" Search main dataframe for hospital reported conditions.\n",
    "\n",
    "    This function uses the following columns of the main dataset:\n",
    "    **41270**\tDiagnoses - ICD10\n",
    "    **41280**\tDate of first in-patient diagnosis - ICD10\n",
    "    **41271**\tDiagnoses - ICD9\n",
    "    **41281**\tDate of first in-patient diagnosis - ICD9\n",
    "    **eid**\n",
    "    Returns a list of 'eid' values that can subsequently be used to retrieve the genetic data of our cohort.\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    pathToMainDataset: str\n",
    "        path to main dataset (csv)\n",
    "    searchCodeDict: dict\n",
    "        dictionary that was created using createCodingDict function\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    hospital_eids: list(str)\n",
    "    \"\"\"\n",
    "\n",
    "    # read dataset\n",
    "\n",
    "    main_dataset = get_columns([\"41270\", \"41280\",\"41271\", \"41281\", \"eid\"], pathToMainDataset)\n",
    "\n",
    "    # create lists for diagnoses and dates for icd9 and icd10. then extract relevant columns from main dataframe\n",
    "\n",
    "    icd9_diag_cols = ['eid']\n",
    "    icd9_date_cols = ['eid']\n",
    "    icd10_diag_cols = ['eid']\n",
    "    icd10_date_cols = ['eid']\n",
    "\n",
    "    icd9_columns = ['eid']\n",
    "    for i in main_dataset.columns:\n",
    "        cstart = str(i).split('-')[0]\n",
    "        if (cstart == '41271'):\n",
    "            icd9_columns.append(i)\n",
    "            icd9_diag_cols.append(i)\n",
    "        elif (cstart == '41281'):\n",
    "            icd9_columns.append(i)\n",
    "            icd9_date_cols.append(i)\n",
    "\n",
    "    hospital_records_icd9 = main_dataset[icd9_columns]\n",
    "\n",
    "    icd10_columns = ['eid']\n",
    "    for i in main_dataset.columns:\n",
    "        cstart = str(i).split('-')[0]\n",
    "        if (cstart == '41270'):\n",
    "            icd10_columns.append(i)\n",
    "            icd10_diag_cols.append(i)\n",
    "        elif (cstart == '41280'):\n",
    "            icd10_columns.append(i)\n",
    "            icd10_date_cols.append(i)\n",
    "\n",
    "    hospital_records_icd10 = main_dataset[icd10_columns]\n",
    "\n",
    "    (icd9_diag_rename, icd9_numbers) = makeRenamingDict(icd9_diag_cols, '.')\n",
    "    (icd9_date_rename, _) = makeRenamingDict(icd9_date_cols, '.')\n",
    "    (icd10_diag_rename, icd10_numbers) = makeRenamingDict(icd10_diag_cols, '.')\n",
    "    (icd10_date_rename, _) = makeRenamingDict(icd10_date_cols, '.')\n",
    "\n",
    "\n",
    "    # steps for icd9 and icd10:\n",
    "    # 1. split dataset into diagnoses and dates\n",
    "    # 2. rename columns to everythong after '.' (works for 412xx columns only, might have to be after '-' for other data columns)\n",
    "    # 3. melt each subset to make data tidy\n",
    "    # 4. set hierarchical index (eid, visit)\n",
    "    # 5. join on these indices\n",
    "    # 6. convert diagnosis code to string and date to datetime object\n",
    "\n",
    "    icd9_diag = hospital_records_icd9[icd9_diag_cols].rename(columns=icd9_diag_rename).melt(id_vars='eid', value_vars=icd9_numbers, value_name='diagnosis', var_name='visit').set_index(['eid', 'visit'])\n",
    "    icd9_date = hospital_records_icd9[icd9_date_cols].rename(columns=icd9_date_rename).melt(id_vars='eid', value_vars=icd9_numbers, value_name='diagnosisDate', var_name='visit').set_index(['eid', 'visit'])\n",
    "    icd9 = icd9_diag.join(icd9_date)\n",
    "    icd9 = icd9.dropna(subset=['diagnosis', 'diagnosisDate'])\n",
    "    icd9.diagnosis = icd9.diagnosis.astype('str')\n",
    "    icd9.diagnosisDate = pd.to_datetime(icd9.diagnosisDate)\n",
    "\n",
    "    icd10_diag = hospital_records_icd10[icd10_diag_cols].rename(columns=icd10_diag_rename).melt(id_vars='eid', value_vars=icd10_numbers, value_name='diagnosis', var_name='visit').set_index(['eid', 'visit'])\n",
    "    icd10_date = hospital_records_icd10[icd10_date_cols].rename(columns=icd10_date_rename).melt(id_vars='eid', value_vars=icd10_numbers, value_name='diagnosisDate', var_name='visit').set_index(['eid', 'visit'])\n",
    "    icd10 = icd10_diag.join(icd10_date)\n",
    "    icd10 = icd10.dropna(subset=['diagnosis', 'diagnosisDate'])\n",
    "    icd10.diagnosis = icd10.diagnosis.astype('str')\n",
    "    icd10.diagnosisDate = pd.to_datetime(icd10.diagnosisDate)\n",
    "\n",
    "    icd9Query = createPandasQueryString(searchCodeDict, 'icd9', columnName = 'diagnosis')\n",
    "    icd10Query = createPandasQueryString(searchCodeDict, 'icd10', columnName = 'diagnosis')\n",
    "\n",
    "    hospital_eids = list(set(icd9.query(icd9Query).append(icd10.query(icd10Query)).reset_index()[['eid']]['eid']))\n",
    "\n",
    "    return hospital_eids\n",
    "\n",
    "\n",
    "def querySelfreportedData(pathToMainDataset: str, pathToCodingFile: str, searchCodeDict: dict) -> list:\n",
    "    \"\"\" Search main dataframe for hospital reported conditions.\n",
    "\n",
    "    This function uses the following columns of the main dataset:\n",
    "    **20002**\tCondition - node_id\n",
    "    **20008**\tYear of reported condition\n",
    "    **20009**\tAge of patient when condition reported\n",
    "    **eid**\n",
    "    Returns a list of 'eid' values that can subsequently be used to retrieve the genetic data of our cohort.\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    pathToMainDataset: str\n",
    "        path to main dataset (csv)\n",
    "    pathToCodingFile: str\n",
    "        path to translation from node_id to ICD10 (tsv)\n",
    "    searchCodeDict: dict\n",
    "        dictionary that was created using createCodingDict function\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    selfreported_eids: list(str)\n",
    "    \"\"\"\n",
    "\n",
    "    # read dataset\n",
    "\n",
    "    main_dataset = pd.read_csv(pathToMainDataset)\n",
    "    coding_dataset = pd.read_csv(pathToCodingFile, delimiter=\"\\t\")\n",
    "\n",
    "\n",
    "    # create lists for condition, year, and age. then extract relevant columns from main dataframe\n",
    "    diag_cols = main_dataset.filter(regex=\"20002-*\").columns.tolist()\n",
    "    diag_cols.append('eid')\n",
    "\n",
    "    year_cols = main_dataset.filter(regex=\"20008-*\").columns.tolist()\n",
    "    year_cols.append('eid')\n",
    "\n",
    "    age_cols = main_dataset.filter(regex=\"20009-*\").columns.tolist()\n",
    "    age_cols.append('eid')\n",
    "\n",
    "    (diag_rename, diag_numbers) = makeRenamingDict(diag_cols, '-')\n",
    "    (year_rename, _) = makeRenamingDict(year_cols, '-')\n",
    "    (age_rename, _) = makeRenamingDict(age_cols, '-')\n",
    "\n",
    "\n",
    "    diag = main_dataset[diag_cols].rename(columns=diag_rename).melt(id_vars='eid', value_vars=diag_numbers, value_name='diagnosis', var_name='visits').set_index(['eid', 'visits'])\n",
    "\n",
    "\n",
    "    # steps for icd9 and icd10:\n",
    "    # 1. split dataset into diagnoses and dates\n",
    "    # 2. rename columns to everythong after '.' (works for 412xx columns only, might have to be after '-' for other data columns)\n",
    "    # 3. melt each subset to make data tidy\n",
    "    # 4. set hierarchical index (eid, visit)\n",
    "    # 5. join on these indices\n",
    "    # 6. convert diagnosis code to string and date to datetime object\n",
    "\n",
    "    diag = main_dataset[diag_cols].rename(columns=diag_rename).melt(id_vars='eid', value_vars=diag_numbers, value_name='diagnosis', var_name='visits').set_index(['eid', 'visits'])\n",
    "    year = main_dataset[year_cols].rename(columns=year_rename).melt(id_vars='eid', value_vars=diag_numbers, value_name='diagnosisYear', var_name='visits').set_index(['eid', 'visits'])\n",
    "    age = main_dataset[age_cols].rename(columns=age_rename).melt(id_vars='eid', value_vars=diag_numbers, value_name='diagnosisAge', var_name='visits').set_index(['eid', 'visits'])\n",
    "\n",
    "    diag_year = diag.join(year)\n",
    "    diag_year_age = diag_year.join(age)\n",
    "    filtered = diag_year_age.dropna(subset=['diagnosis', 'diagnosisYear', 'diagnosisAge'])\n",
    "\n",
    "    icd10Query = createPandasQueryString(searchCodeDict, 'icd10', columnName = 'diagnosis')\n",
    "\n",
    "    selfreported_eids = list(set(filtered.query(icd10Query).reset_index()[['eid']]['eid']))\n",
    "\n",
    "    return selfreported_eids\n",
    "\n",
    "def createPandasQueryString(searchCodeDict: dict, codingType: str, columnName: str) -> str:\n",
    "    \"\"\" Create search query for dataframe given a searchCodeDict.\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    searchCodeDict: dict\n",
    "        dictionary that was created using createCodingDict function\n",
    "    codingType: str\n",
    "        any of the keys in the searchCodeDict\n",
    "        Most likely one of {'read_2', 'read_3', 'icd9', 'icd10'}\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    queryString: str\n",
    "    \"\"\"\n",
    "\n",
    "    queryString = \"\"\n",
    "    for j, item in enumerate(searchCodeDict[codingType]):\n",
    "        if j != 0:\n",
    "            queryString += ' or '\n",
    "        queryString += \"{} == '{}'\".format(columnName, item)\n",
    "    return queryString\n",
    "\n",
    "\n",
    "def createGpClinicalQueryString(searchCodeDict: dict) -> str:\n",
    "    \"\"\" Create search query for UKBB table `gp_clinical` given a searchCodeDict.\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    searchCodeDict: dict\n",
    "        dictionary that was created using createCodingDict function\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    queryString: str\n",
    "\n",
    "    The queryString then needs to be copied to the UKBB data website to download a list of relevant eids\n",
    "    \"\"\"\n",
    "\n",
    "    queryString = 'SELECT distinct eid FROM gp_clinical WHERE '\n",
    "    for i, el in enumerate(['read_2', 'read_3']):\n",
    "        if i != 0:\n",
    "            queryString += ' OR '\n",
    "        queryString += el + ' IN ' + \"(\"\n",
    "        for j, item in enumerate(searchCodeDict[el]):\n",
    "            if j != 0:\n",
    "                queryString += ','\n",
    "            queryString += \"'\" + item + \"'\"\n",
    "        queryString +=  \")\"\n",
    "\n",
    "    return queryString\n",
    "\n",
    "\n",
    "def createCodingDict(pathToLookup: str, searchTerm: str, fuzzy: bool = False, fuzzyNumber: int = 100, fuzzyMatchBetterThan: int = 90) -> (dict, list):\n",
    "    \"\"\" Creates searchCodeDict, a dictionary containing search codes for particular searhc term.\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    pathToLookup: str\n",
    "        path to dataframe with columns ['type', 'code', 'description']\n",
    "    searchTerm: str\n",
    "        search term of disease of interest\n",
    "    fuzzy: {[False], True}\n",
    "        if False: only descriptions containing exact matches are returned\n",
    "        if True: the first _fuzzyNumber_ matches of a fuzzy search are returned\n",
    "    fuzzyNumber: int\n",
    "        number of matches if fuzzy == True\n",
    "    fuzzyMatchBetterThan: int\n",
    "        number between 0 and [100], where 100 = exact match\n",
    "        cutoff criterion for matches if fuzzy == True\n",
    "        default value: 90\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    (searchCodeDict, searhCodeArray)\n",
    "    searchCodeDict: dict\n",
    "        used in other functions\n",
    "    searchCodeArray: list((type: str, code:str, description:str))\n",
    "        not used elsewhere and only returned for sanity check\n",
    "    \"\"\"\n",
    "\n",
    "    lookupDataframe = pd.read_csv(pathToLookup)\n",
    "\n",
    "    if fuzzy == False:\n",
    "        mask = lookupDataframe['description'].str.lower().str.contains(searchTerm, na=False)\n",
    "        codes = lookupDataframe[mask]\n",
    "\n",
    "    if fuzzy == True:\n",
    "        choices = (lookupDataframe['description'].astype(\"str\").str.lower())\n",
    "        matches = process.extract(searchTerm, choices, scorer=fuzz.partial_ratio, limit=fuzzyNumber)\n",
    "        filteredMatches = [m[0] for m in matches if m[1] > fuzzyMatchBetterThan]\n",
    "        codes = lookupDataframe[lookupDataframe['description'].astype(\"str\").str.lower().isin(filteredMatches)]\n",
    "\n",
    "    searchCodeArray = [(i['type'], i['code'], i['description']) for i in codes.iloc()]\n",
    "    searchCodeDict = arrayToDict(searchCodeArray) \n",
    "\n",
    "\n",
    "    return searchCodeDict, searchCodeArray\n",
    "\n",
    "\n",
    "def makeRenamingDict(columnNames: list, delimiter: str) -> (dict, list):\n",
    "    \"\"\"\n",
    "    Creates dictionary for renaming. Used internally only.\n",
    "    \"\"\"\n",
    "    out = dict()\n",
    "    numbers = []\n",
    "    for c in columnNames:\n",
    "        if c == 'eid':\n",
    "            out[c] = 'eid'\n",
    "        else:\n",
    "            out[c] = c.split(delimiter)[-1]\n",
    "            numbers.append(c.split(delimiter)[-1])\n",
    "    return out, numbers\n",
    "\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def _getQuery(datafields:list):\n",
    "    '''\n",
    "    Generate a regex string for a pd.DataFrame.filter function.\n",
    "    :param datafields: any datafield collected from the UKBB Data Showcase website e.g 41270\n",
    "    :return: regex string\n",
    "    :rtype: str'''\n",
    "    start = \"eid|\"\n",
    "    end = ''.join(\"{}-*|\".format(i) for i in datafields)\n",
    "    final = start + end\n",
    "    return final[:-1]\n",
    "\n",
    "\n",
    "def get_columns(datafields: list, maincsv: str, outfile: str=\"\", nowrite:bool=True, write_datafields:bool=False) -> pd.DataFrame():\n",
    "    '''\n",
    "    Generate a dataframe by selecting all relevant columns based on the given datafield(s).\n",
    "    Optionally write the dataframe to a csv file.\n",
    "    :param datafields: any datafield collected from the UKBB Data Showcase website e.g 41270\n",
    "    :param maincsv: the UKBB main dataset in csv format e.g main_ukbbxxxx.csv\n",
    "    :param outfile: name of the csv file to write the new dataframe to\n",
    "    :param nowrite: boolean that determines whether to write the new dataframe to a csv file\n",
    "    :param write_datafields: boolean that determines whether to the relevant columns to a binary file\n",
    "    :return: dataframe\n",
    "    :rtype: pd.DataFrame'''\n",
    "    maindf = pd.read_csv(maincsv, nrows=1, dtype=str)\n",
    "    datafields_query = _getQuery(datafields)\n",
    "    col_list = maindf.filter(regex=datafields_query).columns.tolist()\n",
    "    print(\"Number of selected columns: {}\".format(len(col_list)))\n",
    "    main = pd.read_csv(maincsv, usecols=col_list, dtype=str)\n",
    "    if (nowrite) or (outfile==\"\") :\n",
    "#         print(\"Not writing updated main.csv file\")\n",
    "        return main\n",
    "    else:\n",
    "        print(\"Writing file\")\n",
    "        main.to_csv(outfile, index=False)\n",
    "        return main\n",
    "    if write_datafields:\n",
    "        with open(\"datafields.data\", 'wb') as file:\n",
    "            pickle.dump(col_list)\n",
    "            return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToLookup = '/Users/kiko/IBM/GEN/modellingScripts/isabell/cohortPipeline/lookupCodeDescriptions.csv'\n",
    "pathToCoding = '/Users/kiko/IBM/GEN/modellingScripts/isabell/cohortPipeline/coding19.tsv'\n",
    "pathToMain = '/Users/kiko/IBM/GEN/modellingScripts/isabell/cohortPipeline/ukb41268_head100.csv'\n",
    "searchTerm = 'glaucoma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchCodeDict, searchCodeArray = createCodingDict(pathToLookup=pathToLookup, searchTerm=searchTerm, fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected columns: 161\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eid</th>\n",
       "      <th>20001-0.0</th>\n",
       "      <th>20001-0.1</th>\n",
       "      <th>20001-0.2</th>\n",
       "      <th>20001-0.3</th>\n",
       "      <th>20001-0.4</th>\n",
       "      <th>20001-0.5</th>\n",
       "      <th>20001-1.0</th>\n",
       "      <th>20001-1.1</th>\n",
       "      <th>20001-1.2</th>\n",
       "      <th>...</th>\n",
       "      <th>20002-3.24</th>\n",
       "      <th>20002-3.25</th>\n",
       "      <th>20002-3.26</th>\n",
       "      <th>20002-3.27</th>\n",
       "      <th>20002-3.28</th>\n",
       "      <th>20002-3.29</th>\n",
       "      <th>20002-3.30</th>\n",
       "      <th>20002-3.31</th>\n",
       "      <th>20002-3.32</th>\n",
       "      <th>20002-3.33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1000955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1000969</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1000978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1000981</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1000997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 161 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        eid 20001-0.0 20001-0.1 20001-0.2 20001-0.3 20001-0.4 20001-0.5  \\\n",
       "0   1000015       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "1   1000027       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2   1000039       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "3   1000040       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "4   1000053       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "..      ...       ...       ...       ...       ...       ...       ...   \n",
       "94  1000955       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "95  1000969       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "96  1000978       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "97  1000981       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "98  1000997       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "   20001-1.0 20001-1.1 20001-1.2  ... 20002-3.24 20002-3.25 20002-3.26  \\\n",
       "0        NaN       NaN       NaN  ...        NaN        NaN        NaN   \n",
       "1        NaN       NaN       NaN  ...        NaN        NaN        NaN   \n",
       "2        NaN       NaN       NaN  ...        NaN        NaN        NaN   \n",
       "3        NaN       NaN       NaN  ...        NaN        NaN        NaN   \n",
       "4        NaN       NaN       NaN  ...        NaN        NaN        NaN   \n",
       "..       ...       ...       ...  ...        ...        ...        ...   \n",
       "94       NaN       NaN       NaN  ...        NaN        NaN        NaN   \n",
       "95       NaN       NaN       NaN  ...        NaN        NaN        NaN   \n",
       "96       NaN       NaN       NaN  ...        NaN        NaN        NaN   \n",
       "97       NaN       NaN       NaN  ...        NaN        NaN        NaN   \n",
       "98       NaN       NaN       NaN  ...        NaN        NaN        NaN   \n",
       "\n",
       "   20002-3.27 20002-3.28 20002-3.29 20002-3.30 20002-3.31 20002-3.32  \\\n",
       "0         NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1         NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "2         NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "3         NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "4         NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "..        ...        ...        ...        ...        ...        ...   \n",
       "94        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "95        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "96        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "97        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "98        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "   20002-3.33  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "..        ...  \n",
       "94        NaN  \n",
       "95        NaN  \n",
       "96        NaN  \n",
       "97        NaN  \n",
       "98        NaN  \n",
       "\n",
       "[99 rows x 161 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_columns(['20002', '20001'], pathToMain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryString = createGpClinicalQueryString(searchCodeDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected columns: 521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryHospitalData(pathToMainDataset= pathToMain, searchCodeDict= searchCodeDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium web crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "follow instructions to get the driver here: \n",
    "https://duo.com/decipher/driving-headless-chrome-with-python\n",
    "\n",
    "1. download canary: https://www.google.com/chrome/canary/\n",
    "1. driver download: https://chromedriver.storage.googleapis.com/index.html?path=83.0.4103.14/\n",
    "1. unzip that file\n",
    "1. move driver to a directory and add it that directory to the path (`export PATH=$PATH:~/IBM/GEN/ukbb-cohort/prototype_notebooks/going_headless`)\n",
    "1. execure driver once to make your computer trust it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir going_headless\n",
    "!mv /Users/kiko/Downloads/chromedriver /Users/kiko/IBM/GEN/ukbb-cohort/prototype_notebooks/going_headless/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToDriver = \"/Users/kiko/IBM/GEN/ukbb-cohort/prototype_notebooks/going_headless/chromedriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "applicationId = '51064'\n",
    "userName = 'isabell.kiral@au11.ibm.com'\n",
    "password = '9NYccunFNB4d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* add credentials.py file to gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import credentials\n",
    "\n",
    "def getPayload(queryString, applicationId, username, password):\n",
    "    import os  \n",
    "    from selenium import webdriver  \n",
    "    from selenium.webdriver.common.keys import Keys  \n",
    "    from selenium.webdriver.chrome.options import Options  \n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.common.exceptions import TimeoutException\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "    chrome_options = Options()  \n",
    "    chrome_options.add_argument(\"--headless\")  \n",
    "    chrome_options.binary_location = '/Applications/Google Chrome Canary.app/Contents/MacOS/Google Chrome Canary'\n",
    "    driver = webdriver.Chrome(executable_path=os.path.abspath(pathToDriver), options=chrome_options)  \n",
    "\n",
    "    driver.get(\"https://bbams.ndph.ox.ac.uk/ams/resProjects/dataDownToShowcase?appn_id={}\".format(applicationId))\n",
    "\n",
    "\n",
    "\n",
    "    usernameField = driver.find_element_by_id('id_username')\n",
    "    passwordField = driver.find_element_by_id('id_password')\n",
    "\n",
    "    usernameField.send_keys(username)\n",
    "    passwordField.send_keys(password)\n",
    "\n",
    "    loginButton = driver.find_element_by_id('id_login')\n",
    "    loginButton.click()\n",
    "\n",
    "    dataPortal = driver.find_element_by_link_text(\"1 Data Portal\")\n",
    "    dataPortal.click()\n",
    "\n",
    "    connect = driver.find_element_by_class_name(\"btn_glow\")\n",
    "    connect.click()\n",
    "\n",
    "    sqlField = driver.find_element_by_id('sq0')\n",
    "    sqlField.send_keys(queryString)\n",
    "\n",
    "    fetch = driver.find_element_by_class_name(\"btn_glow\")\n",
    "    fetch.click()\n",
    "\n",
    "    try:\n",
    "        element_present = EC.presence_of_element_located((By.NAME, 'sr'))\n",
    "        WebDriverWait(driver, 10).until(element_present)\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "\n",
    "    hiddenElement = driver.find_element_by_name('sr')\n",
    "    value = hiddenElement.get_property('value')\n",
    "    driver.close()\n",
    "    payload = \"sr=\" + value\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadData(payload: str):\n",
    "    import requests\n",
    "    headers= {\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "    \"accept-language\": \"en-GB,en-US;q=0.9,en;q=0.8\",\n",
    "    \"cache-control\": \"max-age=0\",\n",
    "    \"content-type\": \"application/x-www-form-urlencoded\",\n",
    "    \"sec-fetch-dest\": \"document\",\n",
    "    \"sec-fetch-mode\": \"navigate\",\n",
    "    \"sec-fetch-site\": \"same-site\",\n",
    "    \"sec-fetch-user\": \"?1\",\n",
    "    \"upgrade-insecure-requests\": \"1\"\n",
    "    }\n",
    "    data = payload\n",
    "    url = \"https://biota.ndph.ox.ac.uk/regserv.cgi\"\n",
    "    response = requests.post(url=url, data=data, headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadData(payload: str):\n",
    "    import requests\n",
    "    headers= {\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "    \"accept-language\": \"en-GB,en-US;q=0.9,en;q=0.8\",\n",
    "    \"cache-control\": \"max-age=0\",\n",
    "    \"content-type\": \"application/x-www-form-urlencoded\",\n",
    "    \"sec-fetch-dest\": \"document\",\n",
    "    \"sec-fetch-mode\": \"navigate\",\n",
    "    \"sec-fetch-site\": \"same-site\",\n",
    "    \"sec-fetch-user\": \"?1\",\n",
    "    \"upgrade-insecure-requests\": \"1\"\n",
    "    }\n",
    "    data = payload\n",
    "    url = \"https://biota.ndph.ox.ac.uk/regserv.cgi\"\n",
    "    response = requests.post(url=url, data=data, headers=headers)\n",
    "    return response\n",
    "\n",
    "def extractIds(response: dict):\n",
    "    import io\n",
    "    data = io.StringIO(response.text)\n",
    "    df = pd.read_csv(data, sep=\",\")\n",
    "    return list(df['eid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = getPayload(queryString, credentials.applicationId, credentials.userName, credentials.password)\n",
    "response = downloadData(payload)\n",
    "ids = extractIds(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3256435,\n",
       " 3407919,\n",
       " 4816917,\n",
       " 2496558,\n",
       " 5207925,\n",
       " 4059356,\n",
       " 5292704,\n",
       " 5869589,\n",
       " 1219683,\n",
       " 4076733]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ukbb-cohort import query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchCodeDict, searchCodeArray = query.createCodingDict(pathToLookup, 'glaucoma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Keep '\u001b[1mnot glaucoma\u001b[0m' (\u001b[93mread_2\u001b[0m)? [y/n]:  n\n",
      "Keep '\u001b[1mdefinitely not glaucoma\u001b[0m' (\u001b[92mread_3\u001b[0m)? [y/n]:  n\n",
      "Keep '\u001b[1mearly glaucoma\u001b[0m' (\u001b[92mread_3\u001b[0m)? [y/n]:  n\n",
      "Keep '\u001b[1mwhat is this even listed?\u001b[0m' (\u001b[94micd9\u001b[0m)? [y/n]:  n\n",
      "Keep '\u001b[1mglaucoma\u001b[0m' (\u001b[95micd10\u001b[0m)? [y/n]:  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new code array: []\n"
     ]
    }
   ],
   "source": [
    "searchCodeArray = [(\"read_2\", \"a code\", \"not glaucoma\"),(\"read_3\", \"abc\", \"definitely not glaucoma\"),(\"read_3\", \"A.53482\", \"early glaucoma\"),(\"icd9\", \"COOODE10\", \"what is this even listed?\"),(\"icd10\",\"code green\",\"glaucoma\")]\n",
    "\n",
    "def interactiveFilter(searchCodeArray: list) -> (dict, list):\n",
    "    filteredArray = searchCodeArray.copy()\n",
    "    codeColors = {\n",
    "        \"read_2\": '\\033[93m',\n",
    "        \"read_3\": '\\033[92m',\n",
    "        \"icd9\": '\\033[94m',\n",
    "        \"icd10\": '\\033[95m',\n",
    "        \"default\": '\\033[0m', \n",
    "        \"underline\": '\\033[4m', \n",
    "        \"bold\": '\\033[1m'\n",
    "    }\n",
    "    for el in searchCodeArray: \n",
    "        keep = input(\"Keep '{}{}{}' ({}{}{})? [y/n]: \".format(codeColors['bold'], el[2], codeColors['default'],  codeColors[el[0]], el[0], codeColors['default']))\n",
    "        if keep == 'n': \n",
    "            filteredArray.remove(el)\n",
    "    filteredDict = _searchArrayToDict(filteredArray)\n",
    "    return filteredDict, filteredArray\n",
    "\n",
    "def _searchArrayToDict(searchCodeArray: list) -> dict:\n",
    "    searchCodeDict = dict()\n",
    "    for i in searchCodeArray:\n",
    "        if i[0] in searchCodeDict.keys():\n",
    "            searchCodeDict[i[0]].append('{}'.format(i[1]))\n",
    "        else:\n",
    "            searchCodeDict[i[0]] = ['{}'.format(i[1])]\n",
    "    return searchCodeDict\n",
    "\n",
    "newcodedict, newcodearray = interactiveFilter(searchCodeArray)\n",
    "print(\"new code array: {}\".format(newcodearray))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# allow for more search terms \n",
    "https://github.ibm.com/isabeki/ukbb-cohort/issues/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createCodingDict(pathToLookup: str, searchTerm: str, fuzzy: bool = False, fuzzyNumber: int = 100, fuzzyMatchBetterThan: int = 90) -> (dict, list):\n",
    "    \"\"\" Creates searchCodeDict, a dictionary containing search codes for particular searhc term.\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    pathToLookup: str\n",
    "        path to dataframe with columns ['type', 'code', 'description']\n",
    "    searchTerm: str\n",
    "        search term of disease of interest\n",
    "    fuzzy: {[False], True}\n",
    "        if False: only descriptions containing exact matches are returned\n",
    "        if True: the first _fuzzyNumber_ matches of a fuzzy search are returned\n",
    "    fuzzyNumber: int\n",
    "        number of matches if fuzzy == True\n",
    "    fuzzyMatchBetterThan: int\n",
    "        number between 0 and [100], where 100 = exact match\n",
    "        cutoff criterion for matches if fuzzy == True\n",
    "        default value: 90\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    (searchCodeDict, searhCodeArray)\n",
    "    searchCodeDict: dict\n",
    "    searchCodeArray: list((type: str, code:str, description:str))\n",
    "    \"\"\"\n",
    "\n",
    "    lookupDataframe = pd.read_csv(pathToLookup)\n",
    "\n",
    "    if fuzzy == False:\n",
    "        mask = lookupDataframe['description'].str.lower().str.contains('|'.join(searchTerms), na=False)\n",
    "        codes = lookupDataframe[mask]\n",
    "\n",
    "    if fuzzy == True:\n",
    "        matches = []\n",
    "        choices = (lookupDataframe['description'].astype(\"str\").str.lower())\n",
    "        for searchTerm in searchTerms:\n",
    "            matches.extend(process.extract(searchTerm, choices, scorer=fuzz.partial_ratio, limit=fuzzyNumber))\n",
    "        filteredMatches = [m[0] for m in matches if m[1] > fuzzyMatchBetterThan]\n",
    "        codes = lookupDataframe[lookupDataframe['description'].astype(\"str\").str.lower().isin(filteredMatches)]\n",
    "\n",
    "    searchCodeArray = [(i['type'], i['code'], i['description']) for i in codes.iloc()]\n",
    "    searchCodeDict = _searchArrayToDict(searchCodeArray)\n",
    "    return searchCodeDict, searchCodeArray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchTerms = ['glaucoma', 'psychosis']\n",
    "scd, sca = createCodingDict(pathToLookup, searchTerms, fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('read_2', '115D.', 'No H/O: Glaucoma'),\n",
       " ('read_2', '1221', 'No FH: Glaucoma'),\n",
       " ('read_2', '12A1.', 'FH: Glaucoma'),\n",
       " ('read_2', '146H.', 'H/O: psychosis'),\n",
       " ('read_2', '1482', 'H/O: glaucoma'),\n",
       " ('read_2', '1JF..', 'Suspected glaucoma'),\n",
       " ('read_2',\n",
       "  '212T.',\n",
       "  'Psychosis, schizophrenia and bipolar affective disorder resolved'),\n",
       " ('read_2', '212X.', 'Psychosis resolved'),\n",
       " ('read_2', '2229', 'O/E - senility - no psychosis'),\n",
       " ('read_2', '66T1.', 'Glaucoma monitoring'),\n",
       " ('read_2', '68A2.', 'Glaucoma screen'),\n",
       " ('read_2', '7259', 'Operations following glaucoma surgery'),\n",
       " ('read_2', '72590', 'Needling of bleb following glaucoma surgery'),\n",
       " ('read_2', '72591', 'Injection of bleb following glaucoma surgery'),\n",
       " ('read_2', '72592', 'Revision of bleb NEC following glaucoma surgery'),\n",
       " ('read_2',\n",
       "  '72593',\n",
       "  'Removal of releasable suture following glaucoma surgery'),\n",
       " ('read_2', '72594', 'Laser suture lysis following glaucoma surgery'),\n",
       " ('read_2', '7259y', 'Other specified operations following glaucoma surgery'),\n",
       " ('read_2', '7259z', 'Operations following glaucoma surgery NOS'),\n",
       " ('read_2', '7275', 'Pan retinal photocoagulation for glaucoma'),\n",
       " ('read_2', '8G131', 'CBTp - cognitive behavioural therapy for psychosis'),\n",
       " ('read_2', '8HHs.', 'Referral to psychosis early intervention service'),\n",
       " ('read_2', '8HTW.', 'Referral to glaucoma clinic'),\n",
       " ('read_2', 'E00y.', 'Presbyophrenic psychosis'),\n",
       " ('read_2', 'E0110', \"Korsakov's alcoholic psychosis\"),\n",
       " ('read_2',\n",
       "  'E0111',\n",
       "  \"Korsakov's alcoholic psychosis with peripheral neuritis\"),\n",
       " ('read_2', 'E01y.', 'Other alcoholic psychosis'),\n",
       " ('read_2', 'E01yz', 'Other alcoholic psychosis NOS'),\n",
       " ('read_2', 'E01z.', 'Alcoholic psychosis NOS'),\n",
       " ('read_2', 'E02z.', 'Drug psychosis NOS'),\n",
       " ('read_2', 'E03y3', 'Unspecified puerperal psychosis'),\n",
       " ('read_2', 'E040.', \"Korsakoff's non-alcoholic psychosis\"),\n",
       " ('read_2', 'E04z.', 'Chronic organic psychosis NOS'),\n",
       " ('read_2',\n",
       "  'E1103',\n",
       "  'Single manic episode, severe without mention of psychosis'),\n",
       " ('read_2', 'E1104', 'Single manic episode, severe, with psychosis'),\n",
       " ('read_2',\n",
       "  'E1113',\n",
       "  'Recurrent manic episodes, severe without mention of psychosis'),\n",
       " ('read_2', 'E1114', 'Recurrent manic episodes, severe, with psychosis'),\n",
       " ('read_2',\n",
       "  'E1123',\n",
       "  'Single major depressive episode, severe, without mention of psychosis'),\n",
       " ('read_2',\n",
       "  'E1124',\n",
       "  'Single major depressive episode, severe, with psychosis'),\n",
       " ('read_2',\n",
       "  'E1133',\n",
       "  'Recurrent major depressive episodes, severe, without mention of psychosis'),\n",
       " ('read_2',\n",
       "  'E1134',\n",
       "  'Recurrent major depressive episodes, severe, with psychosis'),\n",
       " ('read_2',\n",
       "  'E1143',\n",
       "  'Bipolar affective disorder, currently manic, severe, without mention of psychosis'),\n",
       " ('read_2',\n",
       "  'E1144',\n",
       "  'Bipolar affective disorder, currently manic, severe, with psychosis'),\n",
       " ('read_2',\n",
       "  'E1153',\n",
       "  'Bipolar affective disorder, currently depressed, severe, without mention of psychosis'),\n",
       " ('read_2',\n",
       "  'E1154',\n",
       "  'Bipolar affective disorder, currently depressed, severe, with psychosis'),\n",
       " ('read_2',\n",
       "  'E1163',\n",
       "  'Mixed bipolar affective disorder, severe, without mention of psychosis'),\n",
       " ('read_2',\n",
       "  'E1164',\n",
       "  'Mixed bipolar affective disorder, severe, with psychosis'),\n",
       " ('read_2',\n",
       "  'E1173',\n",
       "  'Unspecified bipolar affective disorder, severe, without mention of psychosis'),\n",
       " ('read_2',\n",
       "  'E1174',\n",
       "  'Unspecified bipolar affective disorder, severe, with psychosis'),\n",
       " ('read_2', 'E11zz', 'Other affective psychosis NOS'),\n",
       " ('read_2', 'E121.', 'Chronic paranoid psychosis'),\n",
       " ('read_2', 'E12z.', 'Paranoid psychosis NOS'),\n",
       " ('read_2', 'E130.', 'Reactive depressive psychosis'),\n",
       " ('read_2', 'E131.', 'Acute hysterical psychosis'),\n",
       " ('read_2', 'E134.', 'Psychogenic paranoid psychosis'),\n",
       " ('read_2', 'E13y1', 'Brief reactive psychosis'),\n",
       " ('read_2', 'E13z.', 'Nonorganic psychosis NOS'),\n",
       " ('read_2', 'E141.', 'Disintegrative psychosis'),\n",
       " ('read_2', 'E141z', 'Disintegrative psychosis NOS'),\n",
       " ('read_2', 'E14y1', 'Borderline psychosis of childhood'),\n",
       " ('read_2', 'E14z.', 'Child psychosis NOS'),\n",
       " ('read_2', 'E1z..', 'Non-organic psychosis NOS'),\n",
       " ('read_2', 'Eu02z', '[X] Presenile psychosis NOS'),\n",
       " ('read_2', 'Eu02z', '[X] Senile psychosis NOS'),\n",
       " ('read_2', 'Eu03.', \"[X]Korsakov's psychosis, nonalcoholic\"),\n",
       " ('read_2', 'Eu04.', '[X]Acute / subacute infective psychosis'),\n",
       " ('read_2', 'Eu052', '[X]Schizophrenia-like psychosis in epilepsy'),\n",
       " ('read_2', 'Eu05y', '[X]Epileptic psychosis NOS'),\n",
       " ('read_2', 'Eu0z.', '[X]Organic psychosis NOS'),\n",
       " ('read_2', 'Eu0z.', '[X]Symptomatic psychosis NOS'),\n",
       " ('read_2', 'Eu105', '[X]Alcoholic psychosis NOS'),\n",
       " ('read_2', 'Eu106', \"[X]Korsakov's psychosis, alcohol induced\"),\n",
       " ('read_2', 'Eu220', '[X]Paranoid psychosis'),\n",
       " ('read_2', 'Eu230', '[X]Cycloid psychosis'),\n",
       " ('read_2', 'Eu231', '[X]Cycloid psychosis with symptoms of schizophrenia'),\n",
       " ('read_2', 'Eu233', '[X]Psychogenic paranoid psychosis'),\n",
       " ('read_2', 'Eu23z', '[X]Brief reactive psychosis NOS'),\n",
       " ('read_2', 'Eu23z', '[X]Reactive psychosis'),\n",
       " ('read_2', 'Eu250', '[X]Schizoaffective psychosis, manic type'),\n",
       " ('read_2', 'Eu250', '[X]Schizophreniform psychosis, manic type'),\n",
       " ('read_2', 'Eu251', '[X]Schizoaffective psychosis, depressive type'),\n",
       " ('read_2', 'Eu251', '[X]Schizophreniform psychosis, depressive type'),\n",
       " ('read_2', 'Eu252', '[X]Mixed schizophrenic and affective psychosis'),\n",
       " ('read_2', 'Eu25z', '[X]Schizoaffective psychosis NOS'),\n",
       " ('read_2', 'Eu26.', '[X]Nonorganic psychosis in remission'),\n",
       " ('read_2', 'Eu2y.', '[X]Chronic hallucinatory psychosis'),\n",
       " ('read_2', 'Eu2z.', '[X]Unspecified nonorganic psychosis'),\n",
       " ('read_2', 'Eu2z.', '[X]Psychosis NOS'),\n",
       " ('read_2', 'Eu31.', '[X]Manic-depressive psychosis'),\n",
       " ('read_2', 'Eu323', '[X]Single episode of psychogenic depressive psychosis'),\n",
       " ('read_2', 'Eu323', '[X]Single episode of reactive depressive psychosis'),\n",
       " ('read_2',\n",
       "  'Eu329',\n",
       "  '[X]Single major depressive episode, severe, with psychosis, psychosis in remission'),\n",
       " ('read_2',\n",
       "  'Eu32A',\n",
       "  '[X]Recurrent major depressive episodes, severe, with psychosis, psychosis in remission'),\n",
       " ('read_2',\n",
       "  'Eu332',\n",
       "  '[X]Manic-depressive psychosis, depressed type without psychotic symptoms'),\n",
       " ('read_2',\n",
       "  'Eu333',\n",
       "  '[X]Manic-depressive psychosis, depressed type with psychotic symptoms'),\n",
       " ('read_2',\n",
       "  'Eu333',\n",
       "  '[X]Recurrent severe episodes of psychogenic depressive psychosis'),\n",
       " ('read_2',\n",
       "  'Eu333',\n",
       "  '[X]Recurrent severe episodes of reactive depressive psychosis'),\n",
       " ('read_2', 'Eu3z.', '[X]Affective psychosis NOS'),\n",
       " ('read_2', 'Eu44.', '[X]Hysterical psychosis'),\n",
       " ('read_2', 'Eu531', '[X]Puerperal psychosis NOS'),\n",
       " ('read_2', 'Eu840', '[X]Infantile psychosis'),\n",
       " ('read_2', 'Eu841', '[X]Atypical childhood psychosis'),\n",
       " ('read_2', 'Eu843', '[X]Disintegrative psychosis'),\n",
       " ('read_2', 'Eu843', '[X]Symbiotic psychosis'),\n",
       " ('read_2', 'F4042', 'Glaucoma - absolute'),\n",
       " ('read_2', 'F4421', 'Glaucomatocyclitic crises'),\n",
       " ('read_2', 'F45..', 'Glaucoma'),\n",
       " ('read_2', 'F450.', 'Borderline glaucoma'),\n",
       " ('read_2', 'F4500', 'Unspecified preglaucoma'),\n",
       " ('read_2',\n",
       "  'F4501',\n",
       "  'Open angle glaucoma with borderline intraocular pressure'),\n",
       " ('read_2', 'F4502', 'Borderline glaucoma with anatomical narrow angle'),\n",
       " ('read_2', 'F4503', 'Borderline glaucoma steroid responder'),\n",
       " ('read_2', 'F450z', 'Borderline glaucoma NOS'),\n",
       " ('read_2', 'F451.', 'Open-angle glaucoma'),\n",
       " ('read_2', 'F4510', 'Unspecified open-angle glaucoma'),\n",
       " ('read_2', 'F4511', 'Primary open-angle glaucoma'),\n",
       " ('read_2', 'F4511', 'Simple chronic glaucoma'),\n",
       " ('read_2', 'F4512', 'Low tension glaucoma'),\n",
       " ('read_2', 'F4512', 'Normal pressure glaucoma'),\n",
       " ('read_2', 'F4513', 'Pigmentary glaucoma'),\n",
       " ('read_2', 'F4514', 'Glaucoma of childhood'),\n",
       " ('read_2', 'F4515', 'Open-angle glaucoma residual stage'),\n",
       " ('read_2', 'F451z', 'Open-angle glaucoma NOS'),\n",
       " ('read_2', 'F452.', 'Primary angle-closure glaucoma'),\n",
       " ('read_2', 'F452.', 'Closed angle glaucoma'),\n",
       " ('read_2', 'F4520', 'Unspecified primary angle-closure glaucoma'),\n",
       " ('read_2', 'F4521', 'Intermittent primary angle-closure glaucoma'),\n",
       " ('read_2', 'F4522', 'Acute primary angle-closure glaucoma'),\n",
       " ('read_2', 'F4523', 'Chronic primary angle-closure glaucoma'),\n",
       " ('read_2', 'F4524', 'Primary angle-closure glaucoma residual stage'),\n",
       " ('read_2', 'F452z', 'Primary angle-closure glaucoma NOS'),\n",
       " ('read_2', 'F453.', 'Steroid-induced glaucoma'),\n",
       " ('read_2', 'F4530', 'Steroid-induced glaucoma glaucomatous stage'),\n",
       " ('read_2', 'F4531', 'Steroid-induced glaucoma residual stage'),\n",
       " ('read_2', 'F453z', 'Steroid-induced glaucoma NOS'),\n",
       " ('read_2', 'F454.', 'Glaucoma due to disease EC'),\n",
       " ('read_2', 'F4540', 'Glaucoma due to chamber angle anomaly'),\n",
       " ('read_2', 'F4541', 'Glaucoma due to iris anomaly'),\n",
       " ('read_2', 'F4542', 'Glaucoma due to other anterior segment anomaly'),\n",
       " ('read_2', 'F4543', 'Glaucoma due to systemic syndrome'),\n",
       " ('read_2',\n",
       "  'F4544',\n",
       "  'Glaucoma in endocrine, nutritional and metabolic diseases'),\n",
       " ('read_2', 'F454z', 'Glaucoma due to disease NOS'),\n",
       " ('read_2', 'F455.', 'Glaucoma associated with disorders of the lens'),\n",
       " ('read_2', 'F4550', 'Phacolytic glaucoma'),\n",
       " ('read_2', 'F4551', 'Pseudoexfoliation glaucoma'),\n",
       " ('read_2', 'F455z', 'Glaucoma associated with disorders of the lens NOS'),\n",
       " ('read_2', 'F456.', 'Glaucoma associated with other ocular disorders'),\n",
       " ('read_2', 'F4560', 'Glaucoma due to unspecified ocular disorder'),\n",
       " ('read_2', 'F4561', 'Glaucoma due to pupillary block'),\n",
       " ('read_2', 'F4562', 'Glaucoma due to ocular inflammation'),\n",
       " ('read_2', 'F4563', 'Glaucoma due to ocular vascular disorder'),\n",
       " ('read_2', 'F4564', 'Glaucoma due to ocular tumour or cyst'),\n",
       " ('read_2', 'F4564', 'Glaucoma due to ocular cyst'),\n",
       " ('read_2', 'F4565', 'Glaucoma due to ocular trauma'),\n",
       " ('read_2', 'F4566', 'Neovascular glaucoma'),\n",
       " ('read_2', 'F4566', 'Rubeotic glaucoma'),\n",
       " ('read_2', 'F456z', 'Glaucoma associated with other ocular disorders NOS'),\n",
       " ('read_2', 'F45y.', 'Other specified forms of glaucoma'),\n",
       " ('read_2', 'F45y0', 'Hypersecretion glaucoma'),\n",
       " ('read_2', 'F45y1', 'Glaucoma due to episode of increased venous pressure'),\n",
       " ('read_2', 'F45y2', 'Low tension glaucoma'),\n",
       " ('read_2', 'F45yz', 'Other specified glaucoma NOS'),\n",
       " ('read_2', 'F45z.', 'Glaucoma NOS'),\n",
       " ('read_2', 'F4631', 'Glaucomatous subcapsular flecks'),\n",
       " ('read_2', 'F4H14', 'Optic disc glaucomatous atrophy'),\n",
       " ('read_2', 'FyuG.', '[X]Glaucoma'),\n",
       " ('read_2', 'FyuG0', '[X]Other glaucoma'),\n",
       " ('read_2',\n",
       "  'FyuG1',\n",
       "  '[X]Glaucoma in endocrine, nutritional and metabolic diseases classified elsewhere'),\n",
       " ('read_2', 'FyuG2', '[X]Glaucoma in other diseases classified elsewhere'),\n",
       " ('read_2', 'P3200', 'Congenital glaucoma'),\n",
       " ('read_2', 'P3200', 'Newborn glaucoma'),\n",
       " ('read_2', 'Q20y7', 'Traumatic glaucoma due to birth trauma'),\n",
       " ('read_2', 'R20..', '[D]Senility, without mention of psychosis'),\n",
       " ('read_2', 'R20z.', '[D]Senility, without psychosis NOS'),\n",
       " ('read_2', 'ZV111', '[V]Personal history of manic-depressive psychosis'),\n",
       " ('read_2', 'ZV111', '[V]Personal history of manic-depressive psychosis'),\n",
       " ('read_2', 'ZV7A1', '[V]Screening for glaucoma'),\n",
       " ('read_3', '1221', 'No FH: Glaucoma'),\n",
       " ('read_3', '1221', 'No family history of glaucoma'),\n",
       " ('read_3', '12A1.', 'FH: Glaucoma'),\n",
       " ('read_3', '12A1.', 'Family history of glaucoma'),\n",
       " ('read_3', '1482', 'H/O: glaucoma'),\n",
       " ('read_3', '2229', 'O/E - senile - old age (& [no psychosis])'),\n",
       " ('read_3', '2229', 'O/E - senility - no psychosis'),\n",
       " ('read_3', '66T1.', 'Glaucoma monitoring'),\n",
       " ('read_3', '68A2.', 'Glaucoma screening'),\n",
       " ('read_3', '7275', 'Panretinal photocoagulation for glaucoma'),\n",
       " ('read_3',\n",
       "  'E00y.',\n",
       "  '(Other senile and presenile organic psychoses) or (presbyophrenic psychosis)'),\n",
       " ('read_3', 'E00y.', 'Presbyophrenic psychosis'),\n",
       " ('read_3', 'E00z.', '[X]Presenile psychosis NOS'),\n",
       " ('read_3', 'E011.', 'Korsakov psychosis'),\n",
       " ('read_3', 'E011.', 'Korsakoff psychosis'),\n",
       " ('read_3', 'E011.', 'Korsakov alcoholic psychosis'),\n",
       " ('read_3',\n",
       "  'E0111',\n",
       "  \"Korsakov's alcoholic psychosis with peripheral neuritis\"),\n",
       " ('read_3', 'E01y.', 'Other alcoholic psychosis'),\n",
       " ('read_3', 'E01yz', 'Other alcoholic psychosis NOS'),\n",
       " ('read_3', 'E01z.', 'Alcoholic psychosis NOS'),\n",
       " ('read_3', 'E02z.', 'Drug psychosis NOS'),\n",
       " ('read_3', 'E03y3', 'Unspecified puerperal psychosis'),\n",
       " ('read_3', 'E04z.', 'Chronic organic psychosis NOS'),\n",
       " ('read_3',\n",
       "  'E1103',\n",
       "  'Single manic episode, severe without mention of psychosis'),\n",
       " ('read_3', 'E1104', 'Single manic episode, severe, with psychosis'),\n",
       " ('read_3',\n",
       "  'E1113',\n",
       "  'Recurrent manic episodes, severe without mention of psychosis'),\n",
       " ('read_3', 'E1114', 'Recurrent manic episodes, severe, with psychosis'),\n",
       " ('read_3',\n",
       "  'E1123',\n",
       "  'Single major depressive episode, severe, without mention of psychosis'),\n",
       " ('read_3',\n",
       "  'E1124',\n",
       "  'Single major depressive episode, severe, with psychosis'),\n",
       " ('read_3',\n",
       "  'E1133',\n",
       "  'Recurrent major depressive episodes, severe, without mention of psychosis'),\n",
       " ('read_3',\n",
       "  'E1134',\n",
       "  'Recurrent major depressive episodes, severe, with psychosis'),\n",
       " ('read_3',\n",
       "  'E1143',\n",
       "  'Bipolar affective disorder, currently manic, severe, without mention of psychosis'),\n",
       " ('read_3',\n",
       "  'E1144',\n",
       "  'Bipolar affective disorder, currently manic, severe, with psychosis'),\n",
       " ('read_3',\n",
       "  'E1153',\n",
       "  'Bipolar affective disorder, currently depressed, severe, without mention of psychosis'),\n",
       " ('read_3',\n",
       "  'E1154',\n",
       "  'Bipolar affective disorder, currently depressed, severe, with psychosis'),\n",
       " ('read_3',\n",
       "  'E1163',\n",
       "  'Mixed bipolar affective disorder, severe, without mention of psychosis'),\n",
       " ('read_3',\n",
       "  'E1164',\n",
       "  'Mixed bipolar affective disorder, severe, with psychosis'),\n",
       " ('read_3',\n",
       "  'E1173',\n",
       "  'Unspecified bipolar affective disorder, severe, without mention of psychosis'),\n",
       " ('read_3',\n",
       "  'E1174',\n",
       "  'Unspecified bipolar affective disorder, severe, with psychosis'),\n",
       " ('read_3', 'E11zz', 'Other affective psychosis NOS'),\n",
       " ('read_3', 'E121.', 'Chronic paranoid psychosis'),\n",
       " ('read_3', 'E12z.', 'Paranoid psychosis NOS'),\n",
       " ('read_3', 'E130.', 'Reactive depressive psychosis'),\n",
       " ('read_3', 'E131.', 'Acute hysterical psychosis'),\n",
       " ('read_3', 'E134.', 'Psychogenic paranoid psychosis'),\n",
       " ('read_3', 'E13y1', 'Brief reactive psychosis'),\n",
       " ('read_3', 'E13z.', 'Nonorganic psychosis NOS'),\n",
       " ('read_3', 'E141.', 'Disintegrative psychosis'),\n",
       " ('read_3', 'E141z', 'Disintegrative psychosis NOS'),\n",
       " ('read_3', 'E14y1', 'Borderline psychosis of childhood'),\n",
       " ('read_3', 'E14z.', 'Child psychosis NOS'),\n",
       " ('read_3', 'Eu02z', '[X]Presenile psychosis NOS'),\n",
       " ('read_3', 'Eu02z', '[X] Presenile psychosis NOS'),\n",
       " ('read_3', 'Eu02z', '[X] Senile psychosis NOS'),\n",
       " ('read_3', 'Eu04.', '[X]Acute / subacute infective psychosis'),\n",
       " ('read_3', 'Eu052', '[X]Schizophrenia-like psychosis in epilepsy'),\n",
       " ('read_3', 'Eu05y', '[X]Epileptic psychosis NOS'),\n",
       " ('read_3', 'Eu0z.', '[X]Organic psychosis NOS'),\n",
       " ('read_3', 'Eu0z.', '[X]Symptomatic psychosis NOS'),\n",
       " ('read_3', 'Eu105', '[X]Alcoholic psychosis NOS'),\n",
       " ('read_3', 'Eu220', '[X]Paranoid psychosis'),\n",
       " ('read_3', 'Eu230', '[X]Cycloid psychosis'),\n",
       " ('read_3', 'Eu233', '[X]Psychogenic paranoid psychosis'),\n",
       " ('read_3', 'Eu23z', '[X]Brief reactive psychosis NOS'),\n",
       " ('read_3', 'Eu23z', '[X]Reactive psychosis'),\n",
       " ('read_3', 'Eu252', '[X]Mixed schizophrenic and affective psychosis'),\n",
       " ('read_3', 'Eu2y.', '[X]Chronic hallucinatory psychosis'),\n",
       " ('read_3', 'Eu2z.', '[X]Unspecified nonorganic psychosis'),\n",
       " ('read_3', 'Eu2z.', '[X]Psychosis NOS'),\n",
       " ('read_3', 'Eu323', '[X]Single episode of psychogenic depressive psychosis'),\n",
       " ('read_3', 'Eu323', '[X]Single episode of reactive depressive psychosis'),\n",
       " ('read_3',\n",
       "  'Eu332',\n",
       "  '[X]Manic-depressive psychosis, depressed type without psychotic symptoms'),\n",
       " ('read_3',\n",
       "  'Eu333',\n",
       "  '[X]Manic-depressive psychosis, depressed type with psychotic symptoms'),\n",
       " ('read_3',\n",
       "  'Eu333',\n",
       "  '[X]Recurrent severe episodes of psychogenic depressive psychosis'),\n",
       " ('read_3',\n",
       "  'Eu333',\n",
       "  '[X]Recurrent severe episodes of reactive depressive psychosis'),\n",
       " ('read_3', 'Eu3z.', '[X]Affective psychosis NOS'),\n",
       " ('read_3', 'Eu531', '[X]Puerperal psychosis NOS'),\n",
       " ('read_3', 'Eu843', '[X]Disintegrative psychosis'),\n",
       " ('read_3', 'Eu843', '[X]Symbiotic psychosis'),\n",
       " ('read_3', 'F4042', '(Blind hypertensive eye) or (glaucoma absolute)'),\n",
       " ('read_3', 'F4042', 'Absolute glaucoma'),\n",
       " ('read_3', 'F4042', 'Glaucoma - absolute'),\n",
       " ('read_3', 'F4421', 'Glaucomatocyclitic crises'),\n",
       " ('read_3', 'F45..', 'Glaucoma'),\n",
       " ('read_3', 'F450.', 'Borderline glaucoma'),\n",
       " ('read_3', 'F4500', 'Unspecified preglaucoma'),\n",
       " ('read_3', 'F4503', 'Borderline glaucoma steroid responder'),\n",
       " ('read_3', 'F450z', 'Borderline glaucoma NOS'),\n",
       " ('read_3', 'F451.', 'Open-angle glaucoma'),\n",
       " ('read_3', 'F4510', 'Unspecified open-angle glaucoma'),\n",
       " ('read_3', 'F4511', 'Primary open-angle glaucoma'),\n",
       " ('read_3', 'F4512', 'Low tension glaucoma'),\n",
       " ('read_3', 'F4512', 'Normal pressure glaucoma'),\n",
       " ('read_3', 'F4513', 'Pigmentary glaucoma'),\n",
       " ('read_3', 'F4514', 'Glaucoma of childhood'),\n",
       " ('read_3', 'F4515', 'Open-angle glaucoma residual stage'),\n",
       " ('read_3', 'F451z', 'Open-angle glaucoma NOS'),\n",
       " ('read_3', 'F452.', 'Primary angle-closure glaucoma'),\n",
       " ('read_3', 'F452.', 'Closed angle glaucoma'),\n",
       " ('read_3', 'F4520', 'Unspecified primary angle-closure glaucoma'),\n",
       " ('read_3', 'F4524', 'Primary angle-closure glaucoma residual stage'),\n",
       " ('read_3', 'F452z', 'Primary angle-closure glaucoma NOS'),\n",
       " ('read_3', 'F453.', 'Steroid-induced glaucoma'),\n",
       " ('read_3', 'F4530', 'Steroid-induced glaucoma glaucomatous stage'),\n",
       " ('read_3', 'F4531', 'Steroid-induced glaucoma residual stage'),\n",
       " ('read_3', 'F453z', 'Steroid-induced glaucoma NOS'),\n",
       " ('read_3', 'F454.', 'Glaucoma due to disease EC'),\n",
       " ('read_3', 'F4540', 'Glaucoma due to chamber angle anomaly'),\n",
       " ('read_3', 'F4541', 'Glaucoma due to iris anomaly'),\n",
       " ('read_3', 'F4542', 'Glaucoma due to other anterior segment anomaly'),\n",
       " ('read_3', 'F4543', 'Glaucoma due to systemic syndrome'),\n",
       " ('read_3',\n",
       "  'F4544',\n",
       "  'Glaucoma in endocrine, nutritional and metabolic diseases'),\n",
       " ('read_3', 'F454z', 'Glaucoma due to disease NOS'),\n",
       " ('read_3', 'F4550', 'Phacolytic glaucoma'),\n",
       " ('read_3', 'F4551', 'Pseudoexfoliation glaucoma'),\n",
       " ('read_3', 'F455z', 'Glaucoma associated with disorders of the lens NOS'),\n",
       " ('read_3', 'F456.', 'Glaucoma associated with other ocular disorders'),\n",
       " ('read_3', 'F4560', 'Glaucoma due to unspecified ocular disorder'),\n",
       " ('read_3', 'F4563', 'Glaucoma due to ocular vascular disorder'),\n",
       " ('read_3', 'F4564', 'Glaucoma due to ocular tumour or cyst'),\n",
       " ('read_3', 'F4564', 'Glaucoma due to ocular cyst'),\n",
       " ('read_3', 'F4565', 'Glaucoma due to ocular trauma'),\n",
       " ('read_3', 'F456z', 'Glaucoma associated with other ocular disorders NOS'),\n",
       " ('read_3', 'F45y.', 'Other specified forms of glaucoma'),\n",
       " ('read_3', 'F45y0', 'Hypersecretion glaucoma'),\n",
       " ('read_3', 'F45yz', 'Other specified glaucoma NOS'),\n",
       " ('read_3', 'F45z.', 'Glaucoma NOS'),\n",
       " ('read_3', 'F4631', 'Glaucomatous subcapsular flecks'),\n",
       " ('read_3', 'F4H14', 'Optic disc glaucomatous atrophy'),\n",
       " ('read_3', 'FyuG0', '[X]Other glaucoma'),\n",
       " ('read_3',\n",
       "  'FyuG1',\n",
       "  '[X]Glaucoma in endocrine, nutritional and metabolic diseases classified elsewhere'),\n",
       " ('read_3', 'FyuG2', '[X]Glaucoma in other diseases classified elsewhere'),\n",
       " ('read_3', 'P32..', 'Congenital glaucoma'),\n",
       " ('read_3', 'P32..', 'Newborn glaucoma'),\n",
       " ('read_3', 'Q20y7', 'Traumatic glaucoma due to birth trauma'),\n",
       " ('read_3', 'R20..', '[D]Senility, without mention of psychosis'),\n",
       " ('read_3', 'R20z.', '[D]Senility, without psychosis NOS'),\n",
       " ('read_3', 'X00ef', 'Absolute glaucoma'),\n",
       " ('read_3', 'X00em', 'Neovascular glaucoma'),\n",
       " ('read_3', 'X00em', 'Rubeotic glaucoma'),\n",
       " ('read_3', 'X00R0', 'Presbyophrenic psychosis'),\n",
       " ('read_3', 'XaCKs', 'No H/O: Glaucoma'),\n",
       " ('read_3', 'Xad6J', 'CBTp - cognitive behavioural therapy for psychosis'),\n",
       " ('read_3', 'XaJtw', 'Referral to glaucoma clinic'),\n",
       " ('read_3', 'XaL07', 'H/O: psychosis'),\n",
       " ('read_3', 'XaL19', 'Referral to psychosis early intervention service'),\n",
       " ('read_3', 'XaL58', 'Operations following glaucoma surgery'),\n",
       " ('read_3',\n",
       "  'XaL59',\n",
       "  'Removal of releasable suture following glaucoma surgery'),\n",
       " ('read_3', 'XaL5A', 'Laser suture lysis following glaucoma surgery'),\n",
       " ('read_3', 'XaL5B', 'Other specified operations following glaucoma surgery'),\n",
       " ('read_3', 'XaL5C', 'Operations following glaucoma surgery NOS'),\n",
       " ('read_3', 'XaL5m', 'Needling of bleb following glaucoma surgery'),\n",
       " ('read_3', 'XaL5n', 'Injection of bleb following glaucoma surgery'),\n",
       " ('read_3', 'XaL5o', 'Revision of bleb NEC following glaucoma surgery'),\n",
       " ('read_3',\n",
       "  'XaLIa',\n",
       "  'Psychosis, schizophrenia and bipolar affective disorder resolved'),\n",
       " ('read_3', 'XaMwe', 'Psychosis resolved'),\n",
       " ('read_3', 'XaNcY', 'Suspected glaucoma'),\n",
       " ('read_3', 'XaX52', '[X]Nonorganic psychosis in remission'),\n",
       " ('read_3',\n",
       "  'XaX53',\n",
       "  '[X]Single major depressive episode, severe, with psychosis, psychosis in remission'),\n",
       " ('read_3',\n",
       "  'XaX54',\n",
       "  '[X]Recurrent major depressive episodes, severe, with psychosis, psychosis in remission'),\n",
       " ('read_3', 'XE16F', 'Glaucoma due to ocular tumour or cyst'),\n",
       " ('read_3', 'XE18p', 'Borderline glaucoma'),\n",
       " ('read_3', 'XE1aI', 'Korsakov psychosis'),\n",
       " ('read_3', 'XE1gG', '[D]Senility, without mention of psychosis'),\n",
       " ('read_3', 'XE1Y2', 'Chronic paranoid psychosis'),\n",
       " ('read_3', 'XE1Y5', 'Non-organic psychosis NOS'),\n",
       " ('read_3', 'XE1Y6', 'Child psychosis NOS'),\n",
       " ('read_3', 'XE1ZU', '[X]Unspecified nonorganic psychosis'),\n",
       " ('read_3', 'XE2a8', 'Primary angle-closure glaucoma'),\n",
       " ('read_3', 'XM1QA', 'Glaucoma due to ocular cyst'),\n",
       " ('read_3', 'XM1Yd', 'O/E - senility - no psychosis'),\n",
       " ('read_3', 'ZV111', '[V]Personal history of manic-depressive psychosis'),\n",
       " ('read_3', 'ZV7A1', '[V]Screening for glaucoma'),\n",
       " ('icd9', '2984', 'PSYCHOGENIC PARANOID PSYCHOSIS'),\n",
       " ('icd9', '2991', 'DISINTEGRATIVE PSYCHOSIS'),\n",
       " ('icd9', '365', 'GLAUCOMA'),\n",
       " ('icd9', '3650', 'BORDERLINE GLAUCOMA'),\n",
       " ('icd9', '3651', 'OPEN-ANGLE GLAUCOMA'),\n",
       " ('icd9', '3652', 'PRIMARY ANGLE-CLOSURE GLAUCOMA'),\n",
       " ('icd9', '3655', 'GLAUCOMA ASSOCIATED WITH DISORDERS OF THE LENS'),\n",
       " ('icd9', '3656', 'GLAUCOMA ASSOCIATED WITH OTHER OCULAR DISORDERS'),\n",
       " ('icd9', 'V801', 'GLAUCOMA'),\n",
       " ('icd10', 'H40', 'Glaucoma'),\n",
       " ('icd10', 'H40.1', 'Primary open-angle glaucoma'),\n",
       " ('icd10', 'H40.2', 'Primary angle-closure glaucoma'),\n",
       " ('icd10',\n",
       "  'H42.0',\n",
       "  'Glaucoma in endocrine, nutritional and metabolic diseases'),\n",
       " ('icd10', 'Q15.0', 'Congenital glaucoma')]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclusive search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ukbb-cohort as uk\n",
    "import ukbb-cohort.preprocessing as preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def querySelfreportedData(pathToMainDataset: str, pathToCodingFile: str, searchCodeDict: dict, includeCodes: bool = True) -> list:\n",
    "    \"\"\" Search main dataframe for hospital reported conditions.\n",
    "\n",
    "    This function uses the following columns of the main dataset:\n",
    "    **20002**\tCondition - node_id\n",
    "    **20008**\tYear of reported condition\n",
    "    **20009**\tAge of patient when condition reported\n",
    "    **eid**\n",
    "    Returns a list of 'eid' values that can subsequently be used to retrieve the genetic data of our cohort.\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    pathToMainDataset: str\n",
    "        path to main dataset (csv)\n",
    "    pathToCodingFile: str\n",
    "        path to translation from node_id to ICD10 (tsv)\n",
    "    searchCodeDict: dict\n",
    "        dictionary that was created using createCodingDict function\n",
    "    includeCodes: bool [True]\n",
    "        set to False to get the inverse set\n",
    "    Returns:\n",
    "    --------\n",
    "    selfreported_eids: list(str)\n",
    "    \"\"\"\n",
    "\n",
    "    # read dataset\n",
    "\n",
    "#     main_dataset = pd.read_csv(pathToMainDataset)\n",
    "    main_dataset = preprocessing.get_columns([\"20002\", \"20008\",\"20009\", \"eid\"], pathToMainDataset)\n",
    "\n",
    "    coding_dataset = pd.read_csv(pathToCodingFile, delimiter=\"\\t\", dtype=str)\n",
    "\n",
    "\n",
    "    # create lists for condition, year, and age. then extract relevant columns from main dataframe\n",
    "    diag_cols = main_dataset.filter(regex=\"20002-*\").columns.tolist()\n",
    "    diag_cols.append('eid')\n",
    "\n",
    "    year_cols = main_dataset.filter(regex=\"20008-*\").columns.tolist()\n",
    "    year_cols.append('eid')\n",
    "\n",
    "    age_cols = main_dataset.filter(regex=\"20009-*\").columns.tolist()\n",
    "    age_cols.append('eid')\n",
    "\n",
    "    (diag_rename, diag_numbers) = _construct_renaming_dict(diag_cols, '-')\n",
    "    (year_rename, _) = _construct_renaming_dict(year_cols, '-')\n",
    "    (age_rename, _) = _construct_renaming_dict(age_cols, '-')\n",
    "\n",
    "\n",
    "    diag = main_dataset[diag_cols].rename(columns=diag_rename).melt(id_vars='eid', value_vars=diag_numbers, value_name='diagnosis', var_name='visits').set_index(['eid', 'visits'])\n",
    "\n",
    "\n",
    "    # steps for icd9 and icd10:\n",
    "    # 1. split dataset into diagnoses and dates\n",
    "    # 2. rename columns to everythong after '.' (works for 412xx columns only, might have to be after '-' for other data columns)\n",
    "    # 3. melt each subset to make data tidy\n",
    "    # 4. set hierarchical index (eid, visit)\n",
    "    # 5. join on these indices\n",
    "    # 6. convert diagnosis code to string and date to datetime object\n",
    "\n",
    "    diag = main_dataset[diag_cols].rename(columns=diag_rename).melt(id_vars='eid', value_vars=diag_numbers, value_name='diagnosis', var_name='visits').set_index(['eid', 'visits'])\n",
    "    year = main_dataset[year_cols].rename(columns=year_rename).melt(id_vars='eid', value_vars=diag_numbers, value_name='diagnosisYear', var_name='visits').set_index(['eid', 'visits'])\n",
    "    age = main_dataset[age_cols].rename(columns=age_rename).melt(id_vars='eid', value_vars=diag_numbers, value_name='diagnosisAge', var_name='visits').set_index(['eid', 'visits'])\n",
    "\n",
    "    diag_year = diag.join(year)\n",
    "    diag_year_age = diag_year.join(age)\n",
    "    filtered = diag_year_age.dropna(subset=['diagnosis', 'diagnosisYear', 'diagnosisAge'])\n",
    "\n",
    "    all_codes = [item for sublist in [*searchCodeDict.values()] for item in sublist]\n",
    "    translatedCodingDic = _searchCoding19(coding_dataset, all_codes, searchCodeDict)\n",
    "\n",
    "    if not translatedCodingDic['node_ids']:\n",
    "        print(\"No matches found in self-reported data. Returning empty list\")\n",
    "        selfreported_eids = []\n",
    "    else:\n",
    "        icd9Query = _createPandasQueryString(translatedCodingDic, 'node_ids', columnName = 'diagnosis')\n",
    "        selfreported_eids = list(set(filtered.query(icd9Query).reset_index()[['eid']]['eid']))\n",
    "    \n",
    "    if not includeCodes: \n",
    "        selfreported_eids = list(set(main_dataset.query('eid not in {}'.format(selfreported_eids))['eid']))\n",
    "    \n",
    "    return selfreported_eids\n",
    "\n",
    "def queryGpClinicalData(searchCodeDict: dict, pathToCredentials: str, pathToDriver: str,  driverType: str, includeCodes: bool = True) -> list:\n",
    "    \"\"\" Queries UKBB database given a searchCodeDict and returns Eids of matching candidates.\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    searchCodeDict: dict\n",
    "        dictionary that was created using createCodingDict function\n",
    "    pathToCredentials: str\n",
    "    path to a .py file containing the variables:\n",
    "    applicationId: str\n",
    "        ID of the project with UKBB\n",
    "    username: str\n",
    "        UKBB user name\n",
    "    password: str\n",
    "        UKBB password\n",
    "    pathToDriver: str\n",
    "        path to the driver `chromedriver` used by selenium\n",
    "    driverType: str\n",
    "        driverType for selenium e.g chrome or firefox\n",
    "    includeCodes: bool [True]\n",
    "        set to False to get the inverse set\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    eids: list\n",
    "        List of eids matching the search criterion of the searchCodeDict\n",
    "    \"\"\"\n",
    "    supported_drivers = ['chrome', 'firefox']\n",
    "    driverType = driverType.lower()\n",
    "\n",
    "    if not path.exists(pathToCredentials):\n",
    "        sys.exit(\"Credentials file not found\")\n",
    "\n",
    "    if driverType not in supported_drivers:\n",
    "        raise Exception(\"Program only suports {} drivers, you provided {}. Please install relevant driver and browser. Instructions in README.md\".format(supported_drivers, driverType))\n",
    "\n",
    "    from importlib import import_module\n",
    "    cred = import_module(\"credentials\", package=pathToCredentials)\n",
    "\n",
    "    queryString = _createGpClinicalQueryString(searchCodeDict)\n",
    "    payload =  _get_payload(queryString, cred.applicationId, cred.userName, cred.password, pathToDriver, driverType)\n",
    "    response = _download_data(payload)\n",
    "    eids = _extract_eids(response)\n",
    "    \n",
    "    if not includeCodes:\n",
    "        queryString = 'SELECT distinct eid FROM gp_clinical'\n",
    "        payload =  _get_payload(queryString, cred.applicationId, cred.userName, cred.password, pathToDriver, driverType)\n",
    "        response = _download_data(payload)\n",
    "        alleids = _extract_eids(response)\n",
    "        eids = list(set(alleids) - set(eids)) \n",
    "\n",
    "    return eids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryHospitalData(pathToMainDataset: str, searchCodeDict: dict, includeCodes: bool = True) -> list:\n",
    "    \"\"\" Search main dataframe for hospital reported conditions.\n",
    "\n",
    "    This function uses the following columns of the main dataset:\n",
    "    **41270**\tDiagnoses - ICD10\n",
    "    **41280**\tDate of first in-patient diagnosis - ICD10\n",
    "    **41271**\tDiagnoses - ICD9\n",
    "    **41281**\tDate of first in-patient diagnosis - ICD9\n",
    "    **eid**\n",
    "    Returns a list of 'eid' values that can subsequently be used to retrieve the genetic data of our cohort.\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    pathToMainDataset: str\n",
    "        path to main dataset (csv)\n",
    "    searchCodeDict: dict\n",
    "        dictionary that was created using createCodingDict function\n",
    "    includeCodes: bool [True]\n",
    "        set to False to get the inverse set\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    hospital_eids: list(str)\n",
    "    \"\"\"\n",
    "\n",
    "    # read dataset\n",
    "\n",
    "    main_dataset = preprocessing.get_columns([\"41270\", \"41280\",\"41271\", \"41281\", \"eid\"], pathToMainDataset)\n",
    "\n",
    "\n",
    "    # create lists for diagnoses and dates for icd9 and icd10. then extract relevant columns from main dataframe\n",
    "\n",
    "    icd9_diag_cols = ['eid']\n",
    "    icd9_date_cols = ['eid']\n",
    "    icd10_diag_cols = ['eid']\n",
    "    icd10_date_cols = ['eid']\n",
    "\n",
    "    icd9_columns = ['eid']\n",
    "    for i in main_dataset.columns:\n",
    "        cstart = str(i).split('-')[0]\n",
    "        if (cstart == '41271'):\n",
    "            icd9_columns.append(i)\n",
    "            icd9_diag_cols.append(i)\n",
    "        elif (cstart == '41281'):\n",
    "            icd9_columns.append(i)\n",
    "            icd9_date_cols.append(i)\n",
    "\n",
    "    hospital_records_icd9 = main_dataset[icd9_columns]\n",
    "\n",
    "    icd10_columns = ['eid']\n",
    "    for i in main_dataset.columns:\n",
    "        cstart = str(i).split('-')[0]\n",
    "        if (cstart == '41270'):\n",
    "            icd10_columns.append(i)\n",
    "            icd10_diag_cols.append(i)\n",
    "        elif (cstart == '41280'):\n",
    "            icd10_columns.append(i)\n",
    "            icd10_date_cols.append(i)\n",
    "\n",
    "    hospital_records_icd10 = main_dataset[icd10_columns]\n",
    "\n",
    "    (icd9_diag_rename, icd9_numbers) = _construct_renaming_dict(icd9_diag_cols, '.')\n",
    "    (icd9_date_rename, _) = _construct_renaming_dict(icd9_date_cols, '.')\n",
    "    (icd10_diag_rename, icd10_numbers) = _construct_renaming_dict(icd10_diag_cols, '.')\n",
    "    (icd10_date_rename, _) = _construct_renaming_dict(icd10_date_cols, '.')\n",
    "\n",
    "\n",
    "    # steps for icd9 and icd10:\n",
    "    # 1. split dataset into diagnoses and dates\n",
    "    # 2. rename columns to everythong after '.' (works for 412xx columns only, might have to be after '-' for other data columns)\n",
    "    # 3. melt each subset to make data tidy\n",
    "    # 4. set hierarchical index (eid, visit)\n",
    "    # 5. join on these indices\n",
    "    # 6. convert diagnosis code to string and date to datetime object\n",
    "\n",
    "    icd9_diag = hospital_records_icd9[icd9_diag_cols].rename(columns=icd9_diag_rename).melt(id_vars='eid', value_vars=icd9_numbers, value_name='diagnosis', var_name='visit').set_index(['eid', 'visit'])\n",
    "    icd9_date = hospital_records_icd9[icd9_date_cols].rename(columns=icd9_date_rename).melt(id_vars='eid', value_vars=icd9_numbers, value_name='diagnosisDate', var_name='visit').set_index(['eid', 'visit'])\n",
    "    icd9 = icd9_diag.join(icd9_date)\n",
    "    icd9 = icd9.dropna(subset=['diagnosis', 'diagnosisDate'])\n",
    "    icd9.diagnosis = icd9.diagnosis.astype('str')\n",
    "    icd9.diagnosisDate = pd.to_datetime(icd9.diagnosisDate)\n",
    "\n",
    "    icd10_diag = hospital_records_icd10[icd10_diag_cols].rename(columns=icd10_diag_rename).melt(id_vars='eid', value_vars=icd10_numbers, value_name='diagnosis', var_name='visit').set_index(['eid', 'visit'])\n",
    "    icd10_date = hospital_records_icd10[icd10_date_cols].rename(columns=icd10_date_rename).melt(id_vars='eid', value_vars=icd10_numbers, value_name='diagnosisDate', var_name='visit').set_index(['eid', 'visit'])\n",
    "    icd10 = icd10_diag.join(icd10_date)\n",
    "    icd10 = icd10.dropna(subset=['diagnosis', 'diagnosisDate'])\n",
    "    icd10.diagnosis = icd10.diagnosis.astype('str')\n",
    "    icd10.diagnosisDate = pd.to_datetime(icd10.diagnosisDate)\n",
    "\n",
    "    icd9Query = _createPandasQueryString(searchCodeDict, 'icd9', columnName = 'diagnosis')\n",
    "    icd10Query = _createPandasQueryString(searchCodeDict, 'icd10', columnName = 'diagnosis')\n",
    "    \n",
    "    icd9_df = icd9.query(icd9Query)\n",
    "    icd10_df = icd10.query(icd10Query)\n",
    "\n",
    "    hospital_eids = list(set(icd9_df.append(icd10_df).reset_index()[['eid']]['eid']))\n",
    "    \n",
    "    if not includeCodes: \n",
    "        hospital_eids = list(set(main_dataset.query('eid not in {}'.format(hospital_eids))['eid']))\n",
    "\n",
    "    return hospital_eids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToLookup = '/Users/kiko/IBM/GEN/modellingScripts/isabell/cohortPipeline/lookupCodeDescriptions.csv'\n",
    "pathToCoding = '/Users/kiko/IBM/GEN/modellingScripts/isabell/cohortPipeline/coding19.tsv'\n",
    "pathToMainDataset = '/Users/kiko/IBM/GEN/toy-data/ukb41268_datafields_nrows10000.csv'\n",
    "searchTerms = ['glaucoma']\n",
    "\n",
    "pathToCredentials = '.'\n",
    "pathToDriver = \"going_headless/chromedriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchCodeDict, searchCodeArray = uk.query.createCodingDict(pathToLookup=pathToLookup, searchTerms=searchTerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "eids = queryGpClinicalData(searchDict,pathToCredentials, pathToDriver, 'chrome', includeCodes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "noneids = queryGpClinicalData(searchDict,pathToCredentials, pathToDriver, 'chrome', includeCodes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9770\n",
      "220326\n"
     ]
    }
   ],
   "source": [
    "print(len(eids))\n",
    "print(len(noneids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(eids) & set(noneids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230096"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eids) + len(noneids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _searchCoding19(pathToCoding: str, icd10_codes):\n",
    "    \"\"\" Translate icd10 codes to node_ids for selfreported data.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    pathToCoding: str\n",
    "        dictionary that was created using createCodingDict function\n",
    "    icd10_codes: list\n",
    "        list of icd10 codes that need to be translated\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    node_ids: list\n",
    "        list of node_ids \n",
    "\n",
    "    \"\"\"\n",
    "    codingDf = pd.read_csv(pathToCoding, sep='\\t')\n",
    "    codingDf['icd10'] = codingDf['meaning'].apply(lambda x: x.split(' ')[0])\n",
    "    code19Dict = dict(zip(codingDf.icd10, codingDf.node_id))\n",
    "    node_ids = []\n",
    "    for i in icd10_codes:\n",
    "        try:\n",
    "            node_ids.append(code19Dict[i])\n",
    "        except:\n",
    "            pass\n",
    "    return node_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21841 node_ids translated; 610 node_ids could not be translated into icd10 codes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "main_dataset = preprocessing.get_columns([\"20002\", \"20008\",\"20009\", \"eid\"], pathToMainDataset)\n",
    "\n",
    "# create lists for condition, year, and age. then extract relevant columns from main dataframe\n",
    "diag_cols = main_dataset.filter(regex=\"20002-*\").columns.tolist()\n",
    "diag_cols.append('eid')\n",
    "\n",
    "year_cols = main_dataset.filter(regex=\"20008-*\").columns.tolist()\n",
    "year_cols.append('eid')\n",
    "\n",
    "age_cols = main_dataset.filter(regex=\"20009-*\").columns.tolist()\n",
    "age_cols.append('eid')\n",
    "\n",
    "(diag_rename, diag_numbers) = _construct_renaming_dict(diag_cols, '-')\n",
    "(year_rename, _) = _construct_renaming_dict(year_cols, '-')\n",
    "(age_rename, _) = _construct_renaming_dict(age_cols, '-')\n",
    "\n",
    "\n",
    "diag = main_dataset[diag_cols].rename(columns=diag_rename).melt(id_vars='eid', value_vars=diag_numbers, value_name='diagnosis', var_name='visits').set_index(['eid', 'visits'])\n",
    "\n",
    "\n",
    "# steps for icd9 and icd10:\n",
    "# 1. split dataset into diagnoses and dates\n",
    "# 2. rename columns to everythong after '.' (works for 412xx columns only, might have to be after '-' for other data columns)\n",
    "# 3. melt each subset to make data tidy\n",
    "# 4. set hierarchical index (eid, visit)\n",
    "# 5. join on these indices\n",
    "# 6. convert diagnosis code to string and date to datetime object\n",
    "\n",
    "diag = main_dataset[diag_cols].rename(columns=diag_rename).melt(id_vars='eid', value_vars=diag_numbers, value_name='diagnosis', var_name='visits').set_index(['eid', 'visits'])\n",
    "year = main_dataset[year_cols].rename(columns=year_rename).melt(id_vars='eid', value_vars=diag_numbers, value_name='diagnosisYear', var_name='visits').set_index(['eid', 'visits'])\n",
    "age = main_dataset[age_cols].rename(columns=age_rename).melt(id_vars='eid', value_vars=diag_numbers, value_name='diagnosisAge', var_name='visits').set_index(['eid', 'visits'])\n",
    "\n",
    "diag_year = diag.join(year)\n",
    "diag_year_age = diag_year.join(age)\n",
    "filtered = diag_year_age.dropna(subset=['diagnosis', 'diagnosisYear', 'diagnosisAge'])\n",
    "\n",
    "#     translate from node_ids to icd10 codes and overwrite diagnosis column with new value if found, with \"not found\" otherwise \n",
    "codingDf = pd.read_csv(pathToCodingFile, sep='\\t', dtype=str)\n",
    "codingDf['icd10'] = codingDf['meaning'].apply(lambda x: x.split(' ')[0])\n",
    "code19Dict = dict(zip(codingDf.node_id, codingDf.icd10))\n",
    "\n",
    "\n",
    "filtered['diagnosis'] = filtered['diagnosis'].apply(lambda node_id: code19Dict.get(node_id, \"code not found\"))\n",
    "\n",
    "#     if not searchCodeDict['node_ids']:\n",
    "#         print(\"No matches found in self-reported data. Returning empty list\")\n",
    "#         selfreported_eids = []\n",
    "#     else:\n",
    "print(\"{} node_ids translated; {} node_ids could not be translated into icd10 codes\".format(len(filtered.query('not diagnosis == \"code not found\"')), len(filtered.query('diagnosis == \"code not found\"'))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafields=['31','20002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _getQuery(datafields):\n",
    "    '''Generate a regex string for a pd.DataFrame.filter function.\n",
    "    \n",
    "    :param datafields: any datafield collected from the UKBB Data Showcase website e.g 41270\n",
    "    :return: regex string\n",
    "    :rtype: str\n",
    "    '''\n",
    "    \n",
    "    start = r\"^eid\"\n",
    "    end = r''.join(r\"|^{}$|^{}-\".format(i,i) for i in datafields)\n",
    "    final = start + end\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "maindf = pd.read_csv(pathToMainDataset, nrows=1, dtype=str)\n",
    "datafields_query = _getQuery(datafields)\n",
    "col_list = maindf.filter(regex=datafields_query).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'^eid|^31$|^31-|^20002$|^20002-'"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafields_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eid</th>\n",
       "      <th>31-0.0</th>\n",
       "      <th>20002-0.0</th>\n",
       "      <th>20002-0.1</th>\n",
       "      <th>20002-0.2</th>\n",
       "      <th>20002-0.3</th>\n",
       "      <th>20002-0.4</th>\n",
       "      <th>20002-0.5</th>\n",
       "      <th>20002-0.6</th>\n",
       "      <th>20002-0.7</th>\n",
       "      <th>...</th>\n",
       "      <th>20002-3.24</th>\n",
       "      <th>20002-3.25</th>\n",
       "      <th>20002-3.26</th>\n",
       "      <th>20002-3.27</th>\n",
       "      <th>20002-3.28</th>\n",
       "      <th>20002-3.29</th>\n",
       "      <th>20002-3.30</th>\n",
       "      <th>20002-3.31</th>\n",
       "      <th>20002-3.32</th>\n",
       "      <th>20002-3.33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015</td>\n",
       "      <td>1</td>\n",
       "      <td>1065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       eid 31-0.0 20002-0.0 20002-0.1 20002-0.2 20002-0.3 20002-0.4 20002-0.5  \\\n",
       "0  1000015      1      1065       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "  20002-0.6 20002-0.7  ... 20002-3.24 20002-3.25 20002-3.26 20002-3.27  \\\n",
       "0       NaN       NaN  ...        NaN        NaN        NaN        NaN   \n",
       "\n",
       "  20002-3.28 20002-3.29 20002-3.30 20002-3.31 20002-3.32 20002-3.33  \n",
       "0        NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "\n",
       "[1 rows x 138 columns]"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maindf.filter(regex=datafields_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['eid', '31-0.0', '87-0.31', '87-0.33', '87-1.31', '87-1.33', '87-2.31',\n",
       "       '87-2.33', '87-3.31', '87-3.33',\n",
       "       ...\n",
       "       '131994-0.0', '131995-0.0', '131996-0.0', '131997-0.0', '131998-0.0',\n",
       "       '131999-0.0', '132031-0.0', '132033-0.0', '132131-0.0', '132133-0.0'],\n",
       "      dtype='object', length=2904)"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maindf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ukbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToLookup = '/Users/kiko/IBM/GEN/modellingScripts/isabell/cohortPipeline/lookupCodeDescriptions.csv'\n",
    "pathToCoding = '/Users/kiko/IBM/GEN/modellingScripts/isabell/cohortPipeline/coding19.tsv'\n",
    "pathToMainDataset = '/Users/kiko/IBM/GEN/toy-data/ukb41268_datafields_nrows10000.csv'\n",
    "searchTerm = 'glaucoma'\n",
    "\n",
    "pathToCredentials = '.'\n",
    "pathToDriver = \"prototype_notebooks/going_headless/chromedriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (195,197,205,207,307,308,309,774,775,778,779,780,782,783,786,787,788,847,848,849,850,851,852,854,856,857,858,859,1481,1482,1483,1484,1487,1488,1491,1492,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1753,1754,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,2015,2016,2024,2028,2030,2032,2034,2036,2038,2050,2052,2054,2058,2060,2062,2064,2066,2068,2070,2072,2074,2078,2080,2082,2084,2086,2088,2090,2094,2096,2098,2100,2102,2104,2106,2110,2112,2114,2116,2118,2120,2124,2126,2128,2130,2132,2134,2136,2138,2140,2142,2144,2148,2150,2152,2154,2156,2158,2162,2164,2166,2168,2170,2172,2174,2176,2178,2182,2186,2188,2190,2192,2194,2196,2198,2200,2202,2204,2206,2208,2210,2212,2216,2218,2220,2222,2224,2226,2228,2230,2232,2234,2238,2240,2244,2246,2250,2252,2254,2256,2260,2262,2264,2266,2268,2270,2272,2274,2276,2280,2282,2284,2286,2288,2290,2292,2294,2298,2304,2308,2310,2316,2318,2324,2328,2330,2332,2336,2338,2340,2342,2348,2350,2352,2356,2358,2360,2362,2364,2368,2370,2372,2374,2376,2378,2382,2386,2390,2392,2394,2396,2402,2404,2408,2410,2412,2414,2418,2422,2424,2426,2428,2430,2432,2436,2438,2440,2442,2446,2448,2450,2452,2454,2456,2458,2460,2462,2464,2466,2468,2470,2472,2474,2476,2478,2480,2482,2488,2492,2494,2498,2500,2502,2506,2508,2514,2516,2518,2520,2522,2524,2526,2528,2530,2532,2534,2536,2538,2542,2544,2546,2550,2552,2554,2556,2560,2562,2564,2566,2568,2570,2572,2574,2576,2578,2582,2584,2586,2588,2590,2592,2596,2598,2600,2602,2604,2606,2608,2610,2612,2614,2616,2618,2620,2622,2624,2626,2628,2640,2642,2648,2650,2652,2654,2656,2660,2662,2664,2666,2668,2670,2672,2674,2676,2678,2680,2682,2684,2686,2688,2690,2694,2696,2698,2700,2704,2706,2708,2710,2712,2714,2716,2720,2722,2724,2726,2728,2730,2734,2736,2740,2742,2744,2746,2750,2752,2754,2756,2758,2760,2762,2766,2768,2774,2776,2778,2780,2782,2784,2786,2790,2792,2794,2796,2798,2800,2804,2806,2808,2810,2812,2814,2820,2822,2826,2830,2832,2834,2838,2840,2842,2846,2848,2856,2862,2866,2868,2870,2872,2874,2876,2878,2880,2882,2884,2886,2888,2890,2892,2894,2896,2898) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "maindf = pd.read_csv(pathToMainDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = uk.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# round 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
